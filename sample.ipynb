{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip\n",
        "!pip install -U langchain-openai\n",
        "!pip install -U langchain-community\n",
        "!pip install openai\n",
        "!pip install librosa\n",
        "!pip install noisereduce\n",
        "!pip install soundfile\n",
        "!pip install pydub\n",
        "!pip install langchain\n",
        "\n",
        "from openai import OpenAI\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import userdata\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "import noisereduce as nr\n",
        "import soundfile as sf\n",
        "import json\n",
        "from IPython.display import Markdown as md"
      ],
      "metadata": {
        "id": "YvkWpCJJQz5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = userdata.get('openai')\n",
        "\n",
        "client = OpenAI(api_key=key)  # this is also the default, it can be omitted\n",
        "path = 'dir'\n"
      ],
      "metadata": {
        "id": "GZ_UHr9OQ4Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sound_name\n",
        "# sound_name = path.split('/')[-1]\n",
        "# 拡張子削除\n",
        "sound_type = path.split('.')[-1]\n",
        "sound_name = path.split('.')[0]\n",
        "\n",
        "# wavに変換する\n",
        "!ffmpeg -i \"{path}\" \"{sound_name}.wav\"\n",
        "path = f\"{sound_name}.wav\"\n",
        "sound_type = f\"wav\"\n",
        "# 音声ファイルを読み込む\n",
        "audio, rate = librosa.load(path)\n",
        "\n",
        "# ノイズ除去を実行\n",
        "reduced_noise_audio = nr.reduce_noise(y=audio, sr=rate)\n",
        "sf.write(f\"{sound_name}_noisereduce.{sound_type}\", reduced_noise_audio, rate)\n",
        "\n",
        "path = f\"{sound_name}_noisereduce.{sound_type}\""
      ],
      "metadata": {
        "id": "RCe9BNTdRFOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_audio(file_path, segment_length_ms=60000):  # 60,000 ms is 1 minutes\n",
        "    audio = AudioSegment.from_file(file_path)\n",
        "    length_audio = len(audio)\n",
        "    start = 0\n",
        "    end = segment_length_ms\n",
        "    parts = []\n",
        "\n",
        "    while start < length_audio:\n",
        "        # Ensure the end does not exceed the length of the audio\n",
        "        end = min(start + segment_length_ms, length_audio)\n",
        "\n",
        "        # Extract part of the audio\n",
        "        part = audio[start:end]\n",
        "        parts.append(part)\n",
        "\n",
        "        # Move to the next segment\n",
        "        start += segment_length_ms\n",
        "\n",
        "    return parts\n",
        "\n",
        "def whisper(path):\n",
        "    audio_file= open(path, \"rb\")\n",
        "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "    transcript = client.audio.transcriptions.create()\n",
        "    txt=transcript[\"text\"]\n",
        "    return txt\n",
        "\n",
        "def analyze_html_with_gpt(text):\n",
        "    content = f\"\"\"\n",
        "    以下の文章から講義録を作製してください\n",
        "    1. 講義のテーマ\n",
        "    2. 見出し\n",
        "    3. 内容\n",
        "    の階層になるようにマークダウン形式でまとめてください\n",
        "    # 講義:\n",
        "    {text}\n",
        "    # 注意事項\n",
        "    ## 注意\n",
        "    - ```markdown　のようにタグをつける例がありますが不適切です。\n",
        "    - 必ず日本語で記載してください\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": content},\n",
        "        ])\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "bXAnaIuJRKQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_audio_file.m4a' with the path to your m4a file\n",
        "file_path = path\n",
        "audio_parts = split_audio(file_path)\n",
        "# Example of how to export the parts to separate files\n",
        "for i, part in enumerate(audio_parts):\n",
        "    part.export(f\"part_{i}.wav\", format=\"wav\")\n",
        "    # save dir\n",
        "    print(f\"part_{i}_noisereduced.wav\")"
      ],
      "metadata": {
        "id": "6h3mFYpIRSXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqossfZlQl3q"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "for wav in wav_files:\n",
        "    print(f'Now processing {wav}')\n",
        "    audio_file= open(wav, \"rb\")\n",
        "    transcription = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file\n",
        "    )\n",
        "    print(transcription.text)\n",
        "    result.append(transcription.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = analyze_html_with_gpt(result)\n",
        "md(summary)"
      ],
      "metadata": {
        "id": "1IVZvV0ER9-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}